{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zo-f0JziwQq"
      },
      "source": [
        "# Q-learning with FrozenLake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMl0QyV0iwQs"
      },
      "source": [
        "Based on https://github.com/ioarun/openai-gym/blob/master/frozenlake/frozenlake-qlearning.py \n",
        "\n",
        "Environment: https://gym.openai.com/ \n",
        "\n",
        "Details: https://www.kaggle.com/sandovaledwin/q-learning-algorithm-for-solving-frozenlake-game/code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9e2m3GOgiwQu"
      },
      "outputs": [],
      "source": [
        "#Note. You need to install gym! Sometimes difficult on Windows. Google for advise.\n",
        "# pip install gym-toytext should work\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcCnKK6CiwQv"
      },
      "source": [
        "## Problem description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "KB0fUbJyiwQw",
        "outputId": "fe985505-ce55-4b37-b92a-6879a1e6e869"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nThe agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\\n\\nA frozenlake-v0 is a 4x4 grid world that looks as follows:\\nSFFF       \\nFHFH       \\nFFFH       \\nHFFG       \\n\\nMeaning of the letters:\\nS: starting point, safe\\nF: frozen surface, safe\\nH: hole, fall to your doom\\nG: goal, where the frisbee is located\\n\\nThe 16 states (position of the agent): \\nState 0: upper left corner (Start)\\n...\\nState 15: Lower right corner (Goal)\\n\\nThe 4 actions (moves of the agent):\\nLEFT = 0,\\nDOWN = 1,\\nRIGHT = 2,\\nUP = 3.\\n\\nReward:\\nThe episode ends when you reach the goal or fall into the water. \\nYou receive a reward of 1 if you reach the goal, and 0 otherwise.\\n\\nEffect of actions:\\n        def inc(row, col, a):\\n            if a == LEFT:\\n                col = max(col-1,0)\\n            elif a == DOWN:\\n                row = min(row+1,nrow-1)\\n            elif a == RIGHT:\\n                col = min(col+1,ncol-1)\\n            elif a == UP:\\n                row = max(row-1,0)\\n            return (row, col)\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
        "\n",
        "A frozenlake-v0 is a 4x4 grid world that looks as follows:\n",
        "SFFF       \n",
        "FHFH       \n",
        "FFFH       \n",
        "HFFG       \n",
        "\n",
        "Meaning of the letters:\n",
        "S: starting point, safe\n",
        "F: frozen surface, safe\n",
        "H: hole, fall to your doom\n",
        "G: goal, where the frisbee is located\n",
        "\n",
        "The 16 states (position of the agent): \n",
        "State 0: upper left corner (Start)\n",
        "...\n",
        "State 15: Lower right corner (Goal)\n",
        "\n",
        "The 4 actions (moves of the agent):\n",
        "LEFT = 0,\n",
        "DOWN = 1,\n",
        "RIGHT = 2,\n",
        "UP = 3.\n",
        "\n",
        "Reward:\n",
        "The episode ends when you reach the goal or fall into the water. \n",
        "You receive a reward of 1 if you reach the goal, and 0 otherwise.\n",
        "\n",
        "Effect of actions:\n",
        "        def inc(row, col, a):\n",
        "            if a == LEFT:\n",
        "                col = max(col-1,0)\n",
        "            elif a == DOWN:\n",
        "                row = min(row+1,nrow-1)\n",
        "            elif a == RIGHT:\n",
        "                col = min(col+1,ncol-1)\n",
        "            elif a == UP:\n",
        "                row = max(row-1,0)\n",
        "            return (row, col)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AteqW_lFiwQy"
      },
      "source": [
        "## Define environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "64LlDiIpiwQy"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake-v1\",is_slippery=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozOUf3cSiwQz",
        "outputId": "973981e6-bbb2-4fde-a8fe-78fe0a5d938a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/gkreder/opt/miniconda3/envs/MetabM1/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA2b725CiwQ0"
      },
      "source": [
        "## Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZmxkw8viwQ1",
        "outputId": "2411b4c4-cb56-49d5-a02c-848461bf2a74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Sample actions for exploration:\n",
        "env.action_space.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwi4bHO_iwQ2"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXe3i8E_iwQ2",
        "outputId": "a914e5f4-9d6e-47b7-dfec-c069c601c4bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_episodes = 15000 #20000 #60000\n",
        "gamma = 0.95 #0.99\n",
        "learning_rate = 0.7 #0.95 #0.85\n",
        "epsilon = 0.5#1 #0.15 #0.1\n",
        "\n",
        "# initialize the Q table\n",
        "Q = np.zeros([16, 4])\n",
        "Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvBlALTRiwQ3"
      },
      "source": [
        "## Training the Q-table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, {'prob': 1})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5iDnwsPGiwQ3"
      },
      "outputs": [],
      "source": [
        "for _ in range(num_episodes):\n",
        "\tstate, info = env.reset()\n",
        "\tdone = False\n",
        "\twhile done == False:\n",
        "        # First we select an action:\n",
        "\t\tif random.uniform(0, 1) < epsilon: # Flip a skewed coin\n",
        "\t\t\taction = env.action_space.sample() # Explore action space\n",
        "\t\telse:\n",
        "\t\t\taction = np.argmax(Q[state,:]) # Exploit learned values\n",
        "        # Then we perform the action and receive the feedback from the environment\n",
        "\t\t# new_state, reward, done, info = env.step(action)\n",
        "\t\tnew_state, reward, done, truncated , info = env.step(action)\n",
        "        # Finally we learn from the experience by updating the Q-value of the selected action\n",
        "\t\tprediction_error = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
        "\t\tQ[state,action] += learning_rate*prediction_error \n",
        "\t\tstate = new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0IbqlNJiwQ3",
        "outputId": "e3d1ce8d-9425-4e28-e2df-d031ecdf9c91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
              "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
              "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
              "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
              "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
              "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
              "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
              "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V6bw7TfiwQ4"
      },
      "source": [
        "## Sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Jef9ItHTiwQ4",
        "outputId": "f8069808-4c1c-45e7-e08b-2779236bb02d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nLet us sanity check some of the Q-values. \\nFirst we recall what the environment looks like:\\nSFFF       \\nFHFH       \\nFFFH       \\nHFFG       \\n\\nAnd what the 4 actions are:\\nLEFT = 0\\nDOWN = 1\\nRIGHT = 2\\nUP = 3\\n'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Let us sanity check some of the Q-values. \n",
        "First we recall what the environment looks like:\n",
        "SFFF       \n",
        "FHFH       \n",
        "FFFH       \n",
        "HFFG       \n",
        "\n",
        "And what the 4 actions are:\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPVQ587IiwQ5",
        "outputId": "3b09396c-0e75-49a9-c5db-74787935401f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(Q[0])\n",
        "#Should be 1 or 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkeLGTyciwQ5",
        "outputId": "12001cc0-0e09-482b-e3a2-5cad7235a059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(Q[3])\n",
        "#Should be 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt6Ght9TiwQ5",
        "outputId": "4314afaf-d45b-4932-ece9-3a1b1681fdf3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(Q[10])\n",
        "#Should be 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KURwxPhGiwQ6",
        "outputId": "a7bc7883-979e-4295-979a-1481f148029f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(Q[14])\n",
        "#Should be 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFkjYEbkiwQ6"
      },
      "source": [
        "## Using the Q-table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsnxqdoCiwQ6",
        "outputId": "0e3a1c60-e987-4021-f13a-2d3f77026617"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/gkreder/opt/miniconda3/envs/MetabM1/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "# Is our Q good enough to guide us from start to goal without falling into the water?\n",
        "state, info = env.reset()\n",
        "\n",
        "for step in range(10):\n",
        "    env.render()\n",
        "    # Take the action (index) with the maximum expected discounted future reward given that state\n",
        "    action = np.argmax(Q[state,:])\n",
        "    # state, reward, done, info = env.step(action)\n",
        "    state, reward, done, truncated , info = env.step(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "q_learning_intro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
